"""
Select a diverse set of routes that are not deeper than a maximum threshold.
Diversity calculation re-implemented from diversipy package.

Example:
    python select_routes.py --model ../data/chembl_10k_route_distance_model.ckpt


The input and output is a pickled list of an internal route structure format that
is common to many of the setup scripts. 

The script will output the selected routes in JSON-format, the selected targets in
a text file with SMILES strings and the stock compounds in a textfile with SMILES strings
or InChI-keys.
"""
import re
import argparse
import pickle
import json
import torch
import random
from typing import List, Dict, Any, Set
random.seed(3)
import numpy as np
import pandas as pd
from functools import partial
from collections import defaultdict

from route_distances.route_distances import route_distances_calculator
from route_distances.clustering import ClusteringHelper
from route_distances.utils.routes import extract_leaves


def _get_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser("Tool extract non-overlapping routes")
    parser.add_argument(
        "--filename",
        default="uspto/non_overlapping_routes_sorted.pickle",
        help="the path to an output file generated by the 'find_non_overlaps.py' script",
    )
    parser.add_argument(
        "--output",
        default="data/{}-routes.pickle",
        help="the output filename",
    )
    parser.add_argument(
        "--model", help="the path to a LSTM route distance model", default='../data/chembl_10k_route_distance_model.ckpt'
    )
    parser.add_argument(
        "--stock", default="data/{}-stock.txt", help="the filename of the stock file"
    )
    parser.add_argument(
        "--targets", default="data/{}-targets.txt", help="the filename of the targets file"
    )
    parser.add_argument(
        "--ref-routes",
        default="data/{}-routes.json",
        help="the filename of the file with the reference routes",
    )
    parser.add_argument(
        "--size", type=int, default=15000, help="the number of routes to select"
    )
    parser.add_argument(
        "--max-reaction", type=int, default=10, help="the maximum depth of a route"
    )
    parser.add_argument(
        "--stock_kind",
        choices=["smi", "inchi"],
        default="smi",
        help="the format of the stock, either SMILES strings or InChI keys",
    )
    return parser.parse_args()


def _select_routes_greedy_maxmin(distances: np.ndarray, size: int) -> List[int]:
    aggregated_dist_criteria = distances[0, :]
    previous_index = np.argmax(aggregated_dist_criteria)
    selected_indices = [previous_index]
    while len(selected_indices) < size:
        sel_distances = distances[previous_index, :]
        aggregated_dist_criteria = np.minimum(
            aggregated_dist_criteria, sel_distances.ravel()
        )
        previous_index = np.argmax(aggregated_dist_criteria)
        selected_indices.append(previous_index)
    return selected_indices


# assume routes sorted in descending order wrt time
def select_next_routes(min_num_patents, min_size, routes, max_len, maxn=1, scheme="random"):
    patent_ids = set()
    next_idx, next_id = 0, routes[0]["id"].split('@')[0]
    selected_routes, long_routes = [], []

    while len(patent_ids) < min_num_patents or len(selected_routes) < min_size:

        next_routes = []
        for route in routes[next_idx:]:
            pid = route['id'].split('@')[0]
            if pid != next_id:
                patent_ids.add(next_id)  # we fully considered that now
                next_idx = next_idx + len(next_routes)
                next_id = pid
                break
            next_routes += [route]

        random.shuffle(next_routes)
        if scheme == "random":
            next_routes = next_routes[:maxn]
        else:  # max  - shuffle since may have more than 1
            lens = [r['nreactions'] if r['nreactions']<max_len else -1 for r in next_routes ]
            i = lens.index(max(lens))
            next_routes = [next_routes[i]]

        for route in next_routes:
            if route["nreactions"] <= max_len:
                selected_routes += [route]
            else:
                long_routes += [route]

    print("num patents covered:",len(patent_ids))
    return selected_routes, long_routes, next_idx, next_id


def calc_distances (routes, args, size):
    tree_dicts = [route["rt"] for route in routes]
    calculator = route_distances_calculator(model="lstm", model_path=args.model)
    distances = calculator(tree_dicts)
    indices = _select_routes_greedy_maxmin(distances, size)
    return indices


def save_data(selected_routes, args, fid):
    with open(args.output.format(fid), "wb") as fileobj:
        pickle.dump(selected_routes, fileobj)

    count_data = pd.DataFrame(
        {
            "nleaves": [route["nleaves"] for route in selected_routes],
            "nmols": [route["nmols"] for route in selected_routes],
            "nreactions": [route["nreactions"] for route in selected_routes],
            "llr": [route["llr"] for route in selected_routes],
        }
    )
    print(count_data.describe(include="all"))
    branched_routes = [route["llr"] != route["nreactions"] for route in selected_routes]
    print(f"Number of branched routes: {sum(branched_routes)}")

    leaves = set()
    for tree in selected_routes:
        if args.stock_kind == "inchi":
            leaves = leaves.union(tree["leaves"])
        else:
            leaves = leaves.union(extract_leaves(tree["rt"]))

    with open(args.stock.format(fid), "w") as fileobj:
        fileobj.write("\n".join(leaves))

    with open(args.targets.format(fid), "w") as fileobj:
        fileobj.write("\n".join([route["rt"]["smiles"] for route in selected_routes]))

    with open(args.ref_routes.format(fid), "w") as fileobj:
        json.dump([route["rt"] for route in selected_routes], fileobj, indent=4)


# https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside
def atoi(text):
    return int(text) if text.isdigit() else text


def natural_keys(text):
    '''
    alist.sort(key=natural_keys) sorts in human order
    http://nedbatchelder.com/blog/200712/human_sorting.html
    (See Toothy's implementation in the comments)
    '''
    return [atoi(c) for c in re.split(r'(\d+)', text)]


def get_key1(route):
    return route['rt']['children'][0]['metadata']['ID'].split(';')[-1]


def get_key2(route):
    return route['id'].replace('US', '')


# see https://www.uspto.gov/patents/apply/applying-online/patent-number
# still ignore correct (unknown) handling of reissues (RE) and other letters occurring:
# in ids we have ['REE', 'RE', 'B', 'HH', 'H', 'A']
# but year in first component should solve main order
# yields:
# 0 2016 US20160272662A1@4
# 1 2016 US20160272662A1@3
# -1 1976 USRE028973@1
def sort_routes(routes):
    return sorted(routes, key=lambda r: (get_key1(r), natural_keys(get_key2(r))), reverse=True)


def main() -> None:
    args = _get_args()

    with open(args.filename, "rb") as fileobj:
        routes = pickle.load(fileobj)
    print(f"Read {len(routes)}  routes in total")

    # we used this when running this script for the first time
    # routes = sort_routes(routes)
    # with open(args.filename.replace(".pickle","_sorted.pickle"), "wb") as fileobj:
    #     pickle.dump(routes, fileobj)

    nrxn = sum([r["nreactions"] for r in routes])
    print(f"These routes contain {nrxn} reactions")

    min_num_patents, num_routes = 1, args.size  # latter will determine it

    test_routes, test_routesL, next_idx, next_id = select_next_routes(min_num_patents, num_routes,
                                                                      routes, args.max_reaction)
    print(f"Stopped test data extraction at next_idx {next_idx}, next_id {next_id}")
    save_data(test_routesL, args, "l-test")

    valid_routes, valid_routesL, next_idx2, next_id = select_next_routes(min_num_patents, num_routes,
                                                                         routes[next_idx:], args.max_reaction)
    print(f"Stopped valid data extraction at next_idx {next_idx+next_idx2}, next_id {next_id}")
    save_data(valid_routesL, args, "l-valid")

    for fid, routes in [("rt-test", test_routes), ("rt-valid", valid_routes)]:
            print(f"for {fid}, we have {len(routes)} routes to select")

            selected_routes = routes
            nrxn = sum([r["nreactions"] for r in selected_routes])
            print(f"These routes contain {nrxn} reactions")

            save_data(selected_routes, args, fid)

            # to create some specific subsets
            # if "test" in fid:
            #     fvs =list(map(check_uspto50k_train_valid_init, selected_routes))
            #     selected_routes=[r for i,r in enumerate(selected_routes) if fvs[i]]
            #
            #     for fid2, size in [("2k-test", 2000), ("1k-test", 1000)]:
            #
            #         indices = calc_distances(selected_routes, args, size)
            #         print(len(indices), "routes selected", fid2)
            #
            #         selected_routes = [selected_routes[idx] for idx in indices]
            #
            #         nrxn = sum([r["nreactions"] for r in selected_routes])
            #         print(f"These routes contain {nrxn} reactions")
            #
            #         save_data(selected_routes, args, fid2)


if __name__ == "__main__":
    main()

